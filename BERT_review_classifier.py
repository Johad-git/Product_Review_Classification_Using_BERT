# -*- coding: utf-8 -*-
"""miniproject2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_ofsB5om9se9DPttPB1RFPRa3SJcehNt
"""

# Optimized BERT + BiLSTM Classifier for Text Classification
# Includes Hyperparameter Tuning and Enhanced Preprocessing

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW
from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, roc_curve, auc
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
import datetime
from torch.utils.data import DataLoader, TensorDataset, random_split

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load the dataset
df = pd.read_csv('/content/fake reviews dataset.csv')  # Update with actual file path

# Exploratory Data Analysis (EDA)
print("Dataset Info:")
print(df.info())

# Visualizing label distribution
sns.countplot(data=df, x='label', palette='viridis')
plt.title('Distribution of Labels')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# Data Preprocessing
import re
from string import punctuation

def preprocess_data(df):
    # Map labels to binary values
    label_mapping = {"CG": 0, "OR": 1}
    df["label"] = df["label"].map(label_mapping)

    # Keep relevant columns
    df = df[["text_", "label"]]

    # Preprocess text: lowercasing, removing URLs, hashtags, mentions, punctuations
    def clean_text(text):
        text = text.lower()
        text = re.sub(r"http\S+|www\S+|https\S+", "", text)
        text = re.sub(r"@\w+", "", text)
        text = re.sub(r"#", "", text)
        text = re.sub(r"[^\w\s]", "", text)
        return text

    df["text_"] = df["text_"].apply(clean_text)
    return df

df = preprocess_data(df)

# Sampling to balance the dataset
df_class_0 = df[df["label"] == 0].sample(n=20000, random_state=42)
df_class_1 = df[df["label"] == 1].sample(n=20000, random_state=42)
df = pd.concat([df_class_0, df_class_1]).sample(frac=1, random_state=42).reset_index(drop=True)

# Tokenization
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def encode_texts(texts, max_len=128):
    input_ids = []
    attention_masks = []
    for text in texts:
        encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=max_len,
            padding="max_length",
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt"
        )
        input_ids.append(encoded["input_ids"])
        attention_masks.append(encoded["attention_mask"])
    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)

input_ids, attention_masks = encode_texts(df["text_"].values)
labels = torch.tensor(df["label"].values)

# Train-validation split
train_inputs, val_inputs, train_labels, val_labels = train_test_split(
    input_ids, labels, test_size=0.1, random_state=42, stratify=labels
)
train_masks, val_masks = train_test_split(
    attention_masks, test_size=0.1, random_state=42, stratify=labels
)

# Convert to DataLoader
batch_size = 16

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_dataloader = DataLoader(val_data, shuffle=False, batch_size=batch_size)

# Define the model
class BertClassifier(nn.Module):
    def __init__(self):
        super(BertClassifier, self).__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(self.bert.config.hidden_size, 2)

    def forward(self, input_ids, attention_mask):
        bert_output = self.bert(input_ids, attention_mask)
        pooled_output = bert_output.pooler_output
        dropout_output = self.dropout(pooled_output)
        return self.fc(dropout_output)

model = BertClassifier()
model.to(device)

# Optimizer and loss function
optimizer = AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

import time
import datetime  # Ensure datetime is imported for time formatting

# Training the model
epochs = 5
training_stats = []

def format_time(elapsed):
    """
    Takes a time in seconds and returns a string in the format hh:mm:ss.
    """
    elapsed_rounded = int(round(elapsed))
    return str(datetime.timedelta(seconds=elapsed_rounded))

for epoch in range(1, epochs + 1):
    print(f"\nEpoch {epoch}/{epochs}")
    print("-" * 30)
    start_time = time.time()

    # Training phase
    model.train()
    train_loss = 0
    correct_train = 0

    for step, batch in enumerate(train_dataloader):
        input_ids, attention_mask, labels = [b.to(device) for b in batch]

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        train_loss += loss.item()
        loss.backward()
        optimizer.step()

        preds = torch.argmax(outputs, dim=1)
        correct_train += (preds == labels).sum().item()

    avg_train_loss = train_loss / len(train_dataloader)
    train_accuracy = correct_train / len(train_dataloader.dataset)

    # Validation phase
    model.eval()
    val_loss = 0
    correct_val = 0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in val_dataloader:
            input_ids, attention_mask, labels = [b.to(device) for b in batch]
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            preds = torch.argmax(outputs, dim=1)
            correct_val += (preds == labels).sum().item()
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    avg_val_loss = val_loss / len(val_dataloader)
    val_accuracy = correct_val / len(val_dataloader.dataset)

    # Logging metrics
    elapsed_time = format_time(time.time() - start_time)
    print(f"Epoch {epoch} took {elapsed_time}")
    print(f"  Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}")
    print(f"  Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}")

    training_stats.append({
        "epoch": epoch,
        "Train Loss": avg_train_loss,
        "Train Acc": train_accuracy,
        "Val Loss": avg_val_loss,
        "Val Acc": val_accuracy,
        "Time": elapsed_time
    })

# Visualizing the results
epochs = [stat["epoch"] for stat in training_stats]
train_acc = [stat["Train Acc"] for stat in training_stats]
val_acc = [stat["Val Acc"] for stat in training_stats]
train_loss = [stat["Train Loss"] for stat in training_stats]
val_loss = [stat["Val Loss"] for stat in training_stats]

plt.figure(figsize=(12, 6))
plt.plot(epochs, train_acc, marker="o", label="Training Accuracy")
plt.plot(epochs, val_acc, marker="o", label="Validation Accuracy")
plt.title("Training vs Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(epochs, train_loss, marker="o", label="Training Loss")
plt.plot(epochs, val_loss, marker="o", label="Validation Loss")
plt.title("Training vs Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid()
plt.show()

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["CG", "OR"], yticklabels=["CG", "OR"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# Save the trained model and tokenizer
import os

output_dir = "./saved_model/"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

model_save_path = os.path.join(output_dir, "bert_classifier.pt")
tokenizer_save_path = os.path.join(output_dir, "tokenizer/")

# Save model
torch.save(model.state_dict(), model_save_path)
print(f"Model saved to {model_save_path}")

# Save tokenizer
tokenizer.save_pretrained(tokenizer_save_path)
print(f"Tokenizer saved to {tokenizer_save_path}")

# Code to load the model and tokenizer
def load_model_and_tokenizer(model_path, tokenizer_path):
    # Load the tokenizer
    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)

    # Load the model
    model = BertClassifier()
    model.load_state_dict(torch.load(model_path))
    model.to(device)
    model.eval()

    print("Model and tokenizer loaded successfully.")
    return model, tokenizer

# Load the saved model and tokenizer
loaded_model, loaded_tokenizer = load_model_and_tokenizer(model_save_path, tokenizer_save_path)

# Classify a product review given by the user
def classify_review(review, model, tokenizer, max_len=128):
    model.eval()

    # Preprocess the review
    def clean_text(text):
        text = text.lower()
        text = re.sub(r"http\S+|www\S+|https\S+", "", text)
        text = re.sub(r"@\w+", "", text)
        text = re.sub(r"#", "", text)
        text = re.sub(r"[^\w\s]", "", text)
        return text

    review = clean_text(review)

    # Tokenize the review
    encoded = tokenizer.encode_plus(
        review,
        add_special_tokens=True,
        max_length=max_len,
        padding="max_length",
        truncation=True,
        return_attention_mask=True,
        return_tensors="pt"
    )

    input_ids = encoded["input_ids"].to(device)
    attention_mask = encoded["attention_mask"].to(device)

    # Perform inference
    with torch.no_grad():
        outputs = model(input_ids, attention_mask)
        preds = torch.argmax(outputs, dim=1)

    # Map prediction back to label
    label_mapping = {0: "Computer Generated", 1: "Original"}
    prediction = label_mapping[preds.item()]
    return prediction

# User input and classification
user_review = input("Enter a product review: ")
prediction = classify_review(user_review, loaded_model, loaded_tokenizer)
print(f"The review is classified as: {prediction}")